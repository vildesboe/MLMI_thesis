{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File description\n",
    "Create a dataset to use together with manually annotated data; use GPT4 to create explanations for the yes/no annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes about using GPT4 for this task\n",
    "Remarks made from analyzing 20 random samples of the dataset: rows [273, 211, 222,  77, 275, 140, 135, 124, 256,  85,  19, 190, 142, 156, 272, 111, 191,  97, 154,  24]\n",
    "\n",
    "- Explanation may go beyond only the principle in question (eg emotions mentions amends; wanting to better manage anger).\n",
    "- Discussion around that other principle might not be correct (eg 124)\n",
    "- When something is not mentioned in either input story or statement, it tends to explain other things\n",
    "- 24: Statement says \"when an altercation arose between one of my friends and a stranger\". But acc to story, there was no altercation - just \"this guy said something very rude to my friend\".\n",
    "\n",
    "All-in-all GPT-4 was correct with quite good explanations 90% of the time (18/20) (counting the amends which was wrong about emotions as good...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remarks about individual samples\n",
    "- \"273, Good. Gets main point, and also more (for another principle)\"\n",
    "- \"211, Good enough. Not easy to explain why something is coherent, but does well enough. No quotes.\",\n",
    "- \"222, Good. No quotes, but there is 'no misplaced info', so that is not needed either\",\n",
    "- \"77, Good. Acknowledges that input has no emotions, but statement has loads.\",\n",
    "- \"275, Good. Sees plain mistake in statement (no record, which is false)\",\n",
    "- \"140, Good. Explains emotions in story and statement, and how they cohere.\",\n",
    "- \"135, Good. Quotes the main reason for the satisfied principle.\",\n",
    "- \"124, OK. No amends anywhere, BUT says emotions principle is also good, although this is not the case...\",\n",
    "- \"256, Good. Discusses amends although it is in neither story.\",\n",
    "- \"85, Perfect. Concise and 100percent correct. \",\n",
    "- \"19, Good. Sees the most important breach, but also goes slightly beyond\",\n",
    "- \"190, Good enough. Gives one example, but there is more information that this that is unsupported.\",\n",
    "- \"142, Good. (coherence...)\",\n",
    "- \"156, Good. Points to several important quotes that exist both in statement and story.\",\n",
    "- \"272, Good. Explains in general what is similar, but is missing quotes / more specific examples.\",\n",
    "- \"111, No. The not-coherent part is hitting someone unintentionally, but GPT4 does not catch this.\",\n",
    "- \"191, Good. Sees concrete examples - but also misses one bad quote.\",\n",
    "- \"97, Good. Explains what relevant info is there and what irrelevant info is excluded.\",\n",
    "- \"154, Good. (age is broken)\",\n",
    "- \"24, No. Disagrees with the given answer!:O But answer is not obvious...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from key import test_key, miri_key\n",
    "from info import principles, whatCR, rewriteR, yesnoCR_turned\n",
    "import numpy as np\n",
    "import openai\n",
    "import time\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "openai.api_key = miri_key\n",
    "\n",
    "yesnoCR = yesnoCR_turned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/vsb29/.cache/huggingface/datasets/json/default-332cd82ced9b110c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77fcca646b04467db37fd005733e2217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "281"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/home/vsb29/rds/hpc-work/project/data_file1.json\"\n",
    "data = load_dataset(\"json\", data_files=data_path)\n",
    "len(data['train']['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_prompt = \"You will be presented with an input story, a statement, a question about the input story or statement, and the correct answer to the question. Please provide an explanation for why this is the correct answer by using quotes from the input story and statement. Here are a few principles that the statement should satisfy, and they might me useful in your explanation: \"\n",
    "explanations = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(i)\n\u001b[1;32m      4\u001b[0m f\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m###\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(i)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m inp \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m'\u001b[39;49m][i]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m### \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m q \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39minstruction\u001b[39m\u001b[39m'\u001b[39m][i]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39mPlease answer \u001b[39m\u001b[39m'\u001b[39m\u001b[39myes\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mno\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m      7\u001b[0m ans \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m][i]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "f = open(\"explain_GPT4\", \"a\")\n",
    "for i in range(281, 282):\n",
    "    print(i)\n",
    "    f.write(\"\\n###\"+str(i)+\"\\n\")\n",
    "    inp = data['train']['input'][i].split(\"### \")\n",
    "    q = data['train']['instruction'][i].split(\"Please answer 'yes' or 'no'.\")[0].strip()\n",
    "    ans = data['train']['output'][i]\n",
    "    chat_log = [{\"role\": \"system\", \"content\": gpt4_prompt + principles},\n",
    "                {\"role\": \"user\", \"content\": inp[1].strip() + \"\\n\" + inp[2] + \"\\nQUESTION: \" + q + \"\\nANSWER: \"+ans + \"\\nEXPLANATION: \"}]\n",
    "    explanation = answ = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-0613\",\n",
    "            messages=chat_log\n",
    "        )\n",
    "    explanations = np.append(explanations, explanation)\n",
    "    f.write(explanations[i][\"choices\"][0][\"message\"][\"content\"])\n",
    "f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280\n",
      "281\n"
     ]
    }
   ],
   "source": [
    "print(len(explanations))\n",
    "print(len(data['train']['output']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {'input': data['train']['input'], 'question': data['train']['instruction'], 'answer': data['train']['output'], 'explanation': explanations}\n",
    "data_df = pd.DataFrame.from_dict(new_data)\n",
    "json_str = data_df.to_json('data_explanations1.json', orient = 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"explain_GPT4\", \"a\")\n",
    "for i in range(len(data['train']['output'])):\n",
    "    f.write(explanations[i][\"choices\"][0][\"message\"][\"content\"])\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pmix",
   "language": "python",
   "name": "pmix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
